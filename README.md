# UofC Statistical Learning Study Group
Public repository for the UofC Statistical Learning study group. The group will meet over the summer of 2019 for approximately 10 presentations given by university students and staff on different topics. The objective of the group is to develop foundational knowledge and explore new developments in the fields of statistics and machine learning. 

#### Presenting Members:
- Jordan Bannister
- Deepthi Rajashekar
- Anup Tuladhar
- Pauline Mouches
- Anthony Winder
- Lucas Lo Vercio
- Nagesh Subbanna
- Matthias Wilms
- Bryce Besler
- ...

### Meetings
We are tentatively planning to meet wednesday at noon beginning April 3. Lunch will be provided (hopefully). 

### Course Outline

| Date                     | Topic                                          | Presenter |
|:------------------------ |:---------------------------------------------- |:--------- |
| Apr. 3<sup>nd</sup>      | Information Theory                             | Jordan      |
| Apr. 0<sup>th</sup>      | Bayesian Inference                             | David ???       |
| Apr. 0<sup>th</sup>      | Probabilistic Graphical Models                 | Nagesh       |
| Apr. 0<sup>th</sup>      | Multi-level and Mixed Effects Models           | Bryce  |
| Apr. 0<sup>nd</sup>      | Stochastic Processes                           | Deepthi |
| May. 0<sup>nd</sup>      | Recurrent Neural Networks                      | ??? |
| May. 0<sup>nd</sup>      | Reinforcement Learning                         | Anthony |
| May. 0<sup>nd</sup>      | Computational Anatomy                          | Matthias |
| Jun. 0<sup>nd</sup>      | Modern Deep Convolutional Networks             | Pauline |
| Jun. 0<sup>nd</sup>      | Generative Models                              | Anup |
| Jun. 0<sup>nd</sup>      | Geometric Deep Learning                        | ??? |

### Curriculum
The content was selected according to the interests and knowledge of the presenting members. The content was also structured such that foundational lectures are presented early in the summer, and advanced or application focussed lectures are presented later on.  


#### 0 Information Theory (foundational)
Information, entropy (conditional, joint, relative, differential), mutual information, coding theory

#### 1 Bayesian Inference (foundational)
Frequentist vs. Bayesian, bayes theorem, maximum likelihood estimates, maximum a posterori estimates, Markov Chain Monte Carlo (MCMC), Variational Inference (VI)

#### 2 Probabilistic Graphical Models (foundational)
Bayesian networks, markov networks, conditional independence, joint probability factorization, hidden markov models

#### 3 Multi-level and Mixed Effects Models (ANOVA)
Fixed vs random effects, heirarchical regression

#### 4 Stochastic Processes (sequential data)
Random walk (levy process), brownian motion, gaussian Process, markov process, martingale

#### 5 Recurrent Neural Networks (sequential data)
Fully recurrent network, LSTM, training (supervised, reinforcement)

#### 6 Reinforcement Learning (sequential data)
Markov decision process, policy learning (brute force, monte carlo, Q-learning), exploration vs exploitation (multi-armed bandit problem)

#### 7 Computational Anatomy (image data)
Landmarks, diffeomorphisms (morphisms, isomorphisms, homeomorphisms, manifolds), diffeomorphic registration

#### 8 Modern Deep Convolutional Networks (image data)
VGG, ResNet, DenseNet, Batch normalization, Skip connections, Vanishing/Exploding gradients, Shattered gradients.

#### 9 Generative Models (image data)
VAE, GAN, deep belief network, style transfer.

#### 10 Geometric Deep Learning
Graph neural networks, convolution on graphs and manifolds


